{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ML 2","provenance":[],"mount_file_id":"13qzuV_FlCSUwuixDABoWnnne6NW2fTJi","authorship_tag":"ABX9TyO27M6kVQsCS/AFbXTXqkOQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cEhG1gIPSOrN"},"source":["import pandas as pd\n","from collections import Counter\n","from sklearn.datasets import make_classification\n","from imblearn.under_sampling import RandomUnderSampler\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","# getting the dataset from files\n","train_df = pd.read_csv('/content/drive/MyDrive/Programarea-mea/NLP/Project/Datasets/Toxic_dataset/toxic_train.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/Programarea-mea/NLP/Project/Datasets/Toxic_dataset/toxic_test.csv')\n","import os\n","os.chdir('/content/drive/MyDrive/Programarea-mea/NLP/Project')\n","\n","\n","#Merging datasets\n","df = pd.concat([train_df,test_df ])\n","df = df.drop(columns=['Unnamed: 0'])\n","\n","\n","# Balancing data \n","df_toxic  = df[df['toxic']==1]\n","df_non_toxic = df[df['toxic']==0].sample(frac=1)\n","lenght = len(df_toxic)\n","df_balanced = pd.concat([df_toxic, df_non_toxic.iloc[:lenght]])\n","\n","\n","\n","# Split futures/target \n","X = df_balanced['comment_text']\n","X = X.to_list()\n","y = df_balanced['toxic']\n","\n","#Spliting data in test/train parts\n","#X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n","\n","\n","from sklearn.base import BaseEstimator, TransformerMixin # a transformer enherits these classes\n","\n","import nltk\n","from nltk.probability import FreqDist\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","import re\n","\n","# transformer class\n","class TextTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, stopwords, stemmer):\n","        super().__init__()\n","        self.stopwords = stopwords\n","        self.stemmer = stemmer\n","        self.hapaxes = []\n","        self.named_entities = []\n","    \n","    # by default a transformer's fit method just returns self\n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        # for every text in the X list transform its data\n","        for i, sample in enumerate(X):\n","            X[i] = self.__normalize(sample)\n","            X[i] = self.__eliminate_stopwords(X[i])\n","            X[i] = self.__get_hapaxes(X[i])\n","            X[i] = self.__stem(X[i])\n","            self.__named_entity_extraction(X[i])\n","        \n","        # return the transformed text list\n","        return X\n","\n","    def __normalize(self, text):\n","        # select only the words, make them lowercase and eliminate \\r and \\n\n","        return ' '.join(re.findall('[a-zA-Z]+', text.lower().replace(r'\\r', ' ').replace(r'\\n', ' ')))\n","    \n","    def __eliminate_stopwords(self, text):\n","        if self.stopwords is not None:\n","            return ''.join([word for word in text if word not in self.stopwords])\n","    \n","    def __get_hapaxes(self, text, eliminate=True):\n","        # getting the hapaxes\n","        fd = FreqDist(word_tokenize(text))\n","        self.hapaxes.append(fd.hapaxes())\n","                       \n","        # eliminating the hapaxes if needed (by default: needed)\n","        if eliminate:\n","            text = ''.join([word for word in text if word not in self.hapaxes])\n","        \n","        return text\n","    \n","    def __stem(self, text):\n","        return ' '.join([self.stemmer.stem(word) for word in word_tokenize(text)])\n","    \n","    def __named_entity_extraction(self, text):\n","        # tokenize text's sentences\n","        for sent in sent_tokenize(text):\n","            # get the part-of-speech tags of every token of the current sentence\n","            for chunk in nltk.ne_chunk(nltk.pos_tag(word_tokenize(sent))):\n","                # store named entities\n","                if hasattr(chunk, 'label'):\n","                    self.named_entities.append(chunk)\n","\n","\n","\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","stopwords = stopwords.words('english')\n","\n","# create an instance of the transformer class\n","\n","transformer = TextTransformer(stopwords=['and', 'for', 'in', 'about', 'for', 'as', 'the'], stemmer=PorterStemmer())\n","\n","\n","\n","\n","#Transform our data\n","X = transformer.transform(X)\n","print(f'There are {len(X)} samples')\n","\n","\n","\n","from sklearn.metrics import accuracy_score, plot_confusion_matrix\n","from sklearn.pipeline import make_pipeline\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import MaxAbsScaler\n","\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","# creating a pipeline for Logistic Regression\n","lr_pipe = make_pipeline(\n","    TfidfVectorizer(),\n","    MaxAbsScaler(),\n","    LogisticRegression()\n",")\n","\n","# fitting the model\n","lr_pipe.fit(X, y) \n","\n","\n","import joblib\n","\n","filename = 'Toxic_model.pk'\n","joblib.dump(lr_pipe, filename)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECj37hacUmJb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619734080509,"user_tz":-180,"elapsed":507,"user":{"displayName":"Cristian Preguza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCnQ7533uEm2MCkw2x8_EAD4KSLK_Dhs-n0brV=s64","userId":"17242247807725825989"}},"outputId":"2296da8b-7141-46db-e63b-211c10d1ccc2"},"source":["lr_pipe.predict(['nigga'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"LD37IhLNrH4a","executionInfo":{"status":"ok","timestamp":1619789221421,"user_tz":-180,"elapsed":3213,"user":{"displayName":"Cristian Preguza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCnQ7533uEm2MCkw2x8_EAD4KSLK_Dhs-n0brV=s64","userId":"17242247807725825989"}}},"source":["import joblib\n","\n","filename = '/content/drive/MyDrive/Programarea-mea/NLP/Project/Toxic_model.pk'\n","model = joblib.load(filename)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSlj-EJqrPEQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790036244,"user_tz":-180,"elapsed":760,"user":{"displayName":"Cristian Preguza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCnQ7533uEm2MCkw2x8_EAD4KSLK_Dhs-n0brV=s64","userId":"17242247807725825989"}},"outputId":"31d7ea70-3b05-4289-f1fb-14f2d68e3d8f"},"source":["text = ['There is my niga apple']\n","model.predict(text)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"_GJdzp4p9nR2"},"source":[""],"execution_count":null,"outputs":[]}]}